{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a98ef8e-b00e-4182-a49d-cb024fbf66db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "t1 = torch.zeros(5) #zeros and ones\n",
    "#t1\n",
    "#int(t1[2])\n",
    "#float(t1[3])\n",
    "#t1[2] = 6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49538fb-dc9c-4c73-84eb-e15f5e440471",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = torch.tensor([3.6, 5.8, 2.7, 1.0, 4.2, 7.8])\n",
    "#(float(t2[3]), float(t2[4]))\n",
    "\n",
    "t2 = torch.tensor([[3.1, 6.9],[4.8, 4.1],[7.9,3.8]])\n",
    "#float(t2[1][0]), float(t2[1][1])\n",
    "#t2.shape\n",
    "\n",
    "t3 = torch.ones(4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1717c2e1-ce1a-463f-a3d7-01b93cdb8a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 2])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#t2[1:,1]\n",
    "#t2[None] #adds a dimension of size 1\n",
    "t2[None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a73e39df-1732-4ea8-acd2-ebe6e9e38fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t = torch.randn(3,4,4) #random imitation of a 4*4 image\n",
    "#img_t\n",
    "gray_scale_img_t = img_t.mean(-3) # -3 or 0 (in this case): 3rd from last is the RGB dimension\n",
    "#gray_scale_img_t\n",
    "\n",
    "batch_t = torch.randn(5,3,4,4) # imitating a batch of five 4*4 images\n",
    "gray_scale_batch_t = batch_t.mean(-3)\n",
    "\n",
    "wgts = torch.tensor([.21, .71, .08])\n",
    "unsqz_wgts = wgts.unsqueeze(-1).unsqueeze(-1) # shape: 3 --> 3*1*1\n",
    "#img_t, unsqz_wgts, (img_t * unsqz_wgts)\n",
    "weighted_gray_scale_img_t = (img_t * unsqz_wgts).sum(-3)\n",
    "weighted_gray_scale_batch_t = (batch_t * unsqz_wgts).sum(-3)\n",
    "#weighted_gray_scale_batch_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "eda6809f-b3fe-4158-99d0-05203b47d8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts_named = torch.tensor([.21, .71, .08], names=['channels'])\n",
    "#wgts_named\n",
    "\n",
    "img_named = img_t.refine_names(..., 'channels', 'rows', 'columns')\n",
    "#img_named\n",
    "batch_named = batch_t.refine_names(... , 'channels', 'rows', 'columns') \n",
    "#... at the beginning leaves out leading inputs (in this case dimensions)\n",
    "#batch_named.names\n",
    "\n",
    "wgts_aligned = wgts_named.align_as(img_named)\n",
    "wgts_aligned.shape, wgts_aligned.names\n",
    "\n",
    "gray_scale_img_named = (img_named * wgts_aligned).sum('channels')\n",
    "#gray_scale_img_named\n",
    "\n",
    "gray_scale_img_unnamed = gray_scale_img_named.rename(None)\n",
    "#gray_scale_img_unnamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2bf05afd-e8a7-48b5-9ca2-80d6f5245290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4 = torch.tensor([5.6, 3.2])\n",
    "#t4.dtype\n",
    "\n",
    "t5 = torch.zeros(5,8, dtype=torch.double)\n",
    "t6 = t5.int() #half(), float(), double(), short(), int(), long()\n",
    "#t6\n",
    "t7 = t5.to(torch.half) #another way to convert the type\n",
    "(t7 * t6).dtype #implicit casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a808b1cc-9d07-4319-a933-d6717079224a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pts = torch.tensor([[1,2],[3,4],[5,6]])\n",
    "pts_stg = pts.storage() #underlying layout\n",
    "pts_stg[3] = 8\n",
    "#pts #updated!\n",
    "\n",
    "#pts[2].storage_offset()\n",
    "pts.stride()  # (2,1): element (i,j) is at index 2*i + 1*j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "5ed5e90c-0f10-484c-8efe-9923d6e8e143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t8 = torch.ones(3,4)\n",
    "t8.zero_() #Tensor specific function that modifies input (ends with _)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "851cd0ec-8b42-4a89-b000-d605e6f4aed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt3 = pts[2]\n",
    "pt3[1] = 7\n",
    "#pts  #updated!\n",
    "\n",
    "pt2 = pts[1].clone()\n",
    "pt2[1] = 12\n",
    "#pts #not updated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "2509529b-a208-4f5c-8de9-bd543552b1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 1), (1, 3))"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pts_t = pts.t() #transpose for 2d tensors\n",
    "#id(pts.storage()) == id(pts_t.storage()) #same storage\n",
    "#pts_t.stride()\n",
    "\n",
    "t9 = torch.tensor([[1,2,3],[4,5,6]])\n",
    "t10 = t9.t()\n",
    "t9.stride(), t10.stride() #transposing is internally by the change of stride on the same storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "fef6c33c-5d06-4409-a2ab-986ed8e05bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transposing multi-dimensional tensors\n",
    "t11 = torch.randn(2,3,4)\n",
    "t11_t = t11.transpose(0,2) #inputs: 2 indecis on which transpose is applied\n",
    "#t11, t11_t, t11.shape, t11_t.shape, t11.stride(), t11_t.stride(), t11.is_contiguous(), t11_t.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "0ea0008e-5a0f-44fc-a26d-c33aa9e12016",
   "metadata": {},
   "outputs": [],
   "source": [
    "#t9, t10, t9.storage(), t10.storage()\n",
    "t11 = t10.contiguous() #changes storage to be contiguous (and changes stride)\n",
    "#t10.storage(), t11.storage(), t10.shape, t11.shape, t10.stride(), t11.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "1a323938-d6a1-4f52-9262-5ad22bb14bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pts_gpu = pts.to(device='cuda') # move tensor to cuda-based gpu from cpu\n",
    "\n",
    "pts_np = pts.numpy() #interpreting tensor as a numpy array\n",
    "pts_np[1,1] = 9 #updates tensor as well\n",
    "#pts\n",
    "\n",
    "import numpy as np\n",
    "array_np = np.array([1,2,3])\n",
    "t12 = torch.from_numpy(array_np) #interpret a numpy array as a tensor\n",
    "t12[1] = 0 #updates numpy array as well√ü\n",
    "#array_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "d70435b3-b929-406d-8a7b-61607b29438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pts, 'pts.t') #write tensor to a file\n",
    "pts_loaded = torch.load('pts.t') #load a tensor from a file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
