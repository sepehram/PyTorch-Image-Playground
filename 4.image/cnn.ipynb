{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0bb479a-fa17-4c1a-95ef-1c416139edfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 2000\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "dataset_train_t = datasets.CIFAR10('.', train = True, download = False, transform = transforms.ToTensor())\n",
    "dataset_val_t = datasets.CIFAR10('.', train = False, download = False, transform = transforms.ToTensor())\n",
    "\n",
    "imgs = torch.stack([img_t for img_t,_ in dataset_train_t], dim=3)\n",
    "[m1,m2,m3] = imgs.view(3,-1).mean(dim=1).numpy()\n",
    "[s1,s2,s3] = imgs.view(3,-1).std(dim=1).numpy()\n",
    "dataset_train_transformed = datasets.CIFAR10('.', train = True, download = False,\n",
    "                                            transform = transforms.Compose([\n",
    "                                                transforms.ToTensor(),\n",
    "                                                transforms.Normalize((m1,m2,m3), (s1,s2,s3))\n",
    "                                            ]))\n",
    "dataset_val_transformed = datasets.CIFAR10('.', train = False, download = False,\n",
    "                                            transform = transforms.Compose([\n",
    "                                                transforms.ToTensor(),\n",
    "                                                transforms.Normalize((m1,m2,m3), (s1,s2,s3))\n",
    "                                            ]))\n",
    "\n",
    "label_map = {0:0, 2:1}\n",
    "class_names = ['airplane', 'bird']\n",
    "dataset_train = [(img, label_map[label]) for (img, label) in dataset_train_transformed if label in [0,2]]\n",
    "dataset_val = [(img, label_map[label]) for (img, label) in dataset_val_transformed if label in [0,2]]\n",
    "print(len(dataset_train), len(dataset_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a338b9b5-50b1-465b-bca9-1fc1a3336d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 3, 3]) torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "conv= nn.Conv2d(3, 16, kernel_size = 3) # 3 input channels (RGB), 16 output channels, 3*3 kernel size, 1*1 stride\n",
    "print(conv.weight.shape, conv.bias.shape) # bias of conv is the same size as output channels, one bias value per output channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cd36253-2818-457a-abea-4a92a0eec2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32]) torch.Size([1, 16, 30, 30])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 30])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_t, lbl = dataset_train[0]\n",
    "out = conv(img_t.unsqueeze(0)) #conv works on B*C*H*W dimensions so image must be extended on 0th dim\n",
    "print(img_t.shape, out.shape)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(img_t.permute(1,2,0))\n",
    "plt.imshow(out[0,0].detach().numpy()) #try out[0,0], out[0,1] ... out[0,15]\n",
    "print(out[0,0].shape) # 30 * 30 in 0th output channel, no padding reduces the size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e90adefa-3bab-46a8-84df-bb583e907534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x113277af0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 16, kernel_size = 3, padding = 1) #add 1 pixel to the border of images\n",
    "out = conv(img_t.unsqueeze(0))\n",
    "print(out.shape)\n",
    "plt.imshow(out[0,0].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36feaa6e-2acb-492f-b5b8-9debcafc7627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "#downsampling\n",
    "pool = nn.MaxPool2d(2) #2*2 max pooling\n",
    "print(pool(img_t).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab323a0e-8dde-4cc8-879f-8faea2d9a183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[432, 16, 1152, 8, 16384, 32, 64, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0715, -0.0536]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input: 1 * 3 * 32 * 32\n",
    "model = nn.Sequential(nn.Conv2d(3, 16, kernel_size=3, padding=1), #output: 1 * 16 * 32 * 32\n",
    "                     nn.Tanh(), #output: 1 * 16 * 32 * 32\n",
    "                     nn.MaxPool2d(2), #output: 1 * 16 * 16 * 16\n",
    "                     nn.Conv2d(16, 8, kernel_size=3, padding=1), #output: 1 * 8 * 16 * 16\n",
    "                     nn.Tanh(), #output: 1 * 8 * 16 * 16\n",
    "                     nn.MaxPool2d(2), #output: 1 * 8 * 8 * 8\n",
    "                     nn.Flatten(), # output: 1 * 512\n",
    "                     nn.Linear(8*8*8, 32), #output: 1 * 32\n",
    "                     nn.Tanh(), #output: 1 * 32\n",
    "                     nn.Linear(32,2) # output: 1 * 2\n",
    "                     )\n",
    "print([p.numel() for p in model.parameters()]) #1st Conv2d: 16*3*3*3 weights, 16 biases\n",
    "                                               #2nd Conv2: 8*16*3*3 weights, 8 biases\n",
    "                                               #1st Linear: 8*8*8*32 weights, 32 biases\n",
    "                                               #2nd Linear: 32*2 wieghts, 2 biases \n",
    "model(img_t.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf9c20c0-dfa2-4c74-8239-25468f2c4fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[432, 16, 1152, 8, 16384, 32, 64, 2]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a nn.Module subclass with user-defined functionalities\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size = 3, padding = 1)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size = 3, padding = 1)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(512, 32)\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(32,2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = self.pool1(self.act1(self.conv1(x)))\n",
    "        out = self.pool2(self.act2(self.conv2(out)))\n",
    "        out.view(-1, 8*8*8) # does the flattening: 1 * 512\n",
    "        out = self.fc2(self.act3(self.fc1(out)))\n",
    "        return out\n",
    "\n",
    "model = Net()\n",
    "[p.numel() for p in model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "182901a8-5f3d-41d9-a787-06cc94448a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[432, 16, 1152, 8, 16384, 32, 64, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1187, -0.2929]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nn.Tanh() and nn.MaxPool2d do not have parameters to be trained. \n",
    "#Using functional to define them as functions in the submodule\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size = 3, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size = 3, padding = 1)\n",
    "        self.fc1 = nn.Linear(512,32)\n",
    "        self.fc2 = nn.Linear(32,2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh((self.conv1(x))), 2)\n",
    "        out = F.max_pool2d(torch.tanh((self.conv2(out))), 2)\n",
    "        out = out.view(-1, 8*8*8)\n",
    "        out = self.fc2(torch.tanh(self.fc1(out)))\n",
    "        return out\n",
    "\n",
    "model = Net()\n",
    "print([p.numel() for p in model.parameters()])\n",
    "\n",
    "model(img_t.unsqueeze(0))\n",
    "#img_t.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1d64e88-1dfa-4c4c-b056-48b1459a5cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def training_loop(n_epochs, model, loss_fn, optimizer, train_loader):\n",
    "    for epoch in range(1 + n_epochs):\n",
    "        train_loss = 0.0\n",
    "        for (imgs, lbls) in train_loader:\n",
    "            outs = model(imgs) \n",
    "            loss = loss_fn(outs, lbls)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        if epoch%10 == 0:\n",
    "            print('{} Epoch {}, train loss {}'.format(\n",
    "                datetime.datetime.now(), epoch, train_loss/len(train_loader)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34db2c9c-80f4-42ce-85e6-2f26a3941f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 19:08:58.504900 Epoch 0, train loss 0.5903032304375035\n",
      "2022-04-04 19:09:19.703363 Epoch 10, train loss 0.33278256929983757\n",
      "2022-04-04 19:09:40.917561 Epoch 20, train loss 0.280005200748231\n",
      "2022-04-04 19:10:02.079043 Epoch 30, train loss 0.2561660957089655\n",
      "2022-04-04 19:10:23.354073 Epoch 40, train loss 0.23959908008005967\n",
      "2022-04-04 19:10:44.552544 Epoch 50, train loss 0.2246609479188919\n",
      "2022-04-04 19:11:05.907265 Epoch 60, train loss 0.21028359163149146\n",
      "2022-04-04 19:11:27.454657 Epoch 70, train loss 0.19633876437404355\n",
      "2022-04-04 19:11:48.702621 Epoch 80, train loss 0.18269393213425472\n",
      "2022-04-04 19:12:10.046013 Epoch 90, train loss 0.16901339846811478\n",
      "2022-04-04 19:12:31.198781 Epoch 100, train loss 0.1552509972529047\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(dataset_val, batch_size=64, shuffle=False)\n",
    "\n",
    "model = Net()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-2)\n",
    "\n",
    "training_loop(n_epochs = 100,\n",
    "              model = model,\n",
    "              loss_fn = loss_fn,\n",
    "              optimizer = optimizer,\n",
    "              train_loader = train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acd760f3-9e21-4eb7-964b-ad973fa25d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy train: 0.9294\n",
      "accuracy val: 0.899\n"
     ]
    }
   ],
   "source": [
    "def validate(model, train_loader, val_loader):\n",
    "    for (name, loader) in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for (imgs, lbls) in loader:\n",
    "                outs = model(imgs)\n",
    "                _, predicted = torch.max(outs, dim=1) #index of max is used, not the max probability\n",
    "                total += lbls.shape[0] # number of examples is accumulated\n",
    "                correct += int((predicted == lbls).sum())\n",
    "        print(\"accuracy {}: {}\".format(name, correct/total))\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56541ac7-20df-484c-bc88-4bcd4325d298",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './birds_airplanes_model.pt') #only parameters are saved, not the network structure itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2d1d957-a214-4aec-9952-7388265eda5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model = Net()\n",
    "load_model.load_state_dict(torch.load('./birds_airplanes_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8568d373-5cc5-40eb-ae13-10e2dd649a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "2022-04-04 19:13:21.041762 Epoch 0, train loss 0.5824354575697783\n",
      "2022-04-04 19:13:42.068858 Epoch 10, train loss 0.32963660701065306\n",
      "2022-04-04 19:14:03.256260 Epoch 20, train loss 0.29404123696931606\n",
      "2022-04-04 19:14:24.402660 Epoch 30, train loss 0.26820156633094616\n",
      "2022-04-04 19:14:45.468565 Epoch 40, train loss 0.24787293554870946\n",
      "2022-04-04 19:15:06.707774 Epoch 50, train loss 0.2301603423277284\n",
      "2022-04-04 19:15:27.932138 Epoch 60, train loss 0.21359395591696356\n",
      "2022-04-04 19:15:49.069910 Epoch 70, train loss 0.1973653700511167\n",
      "2022-04-04 19:16:10.179883 Epoch 80, train loss 0.18152965787964262\n",
      "2022-04-04 19:16:31.679216 Epoch 90, train loss 0.16600072353034262\n",
      "2022-04-04 19:16:53.589395 Epoch 100, train loss 0.1508954973642234\n"
     ]
    }
   ],
   "source": [
    "dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(dev)\n",
    "\n",
    "def training_loop(n_epochs, model, loss_fn, optimizer, train_loader):\n",
    "    for epoch in range(1 + n_epochs):\n",
    "        train_loss = 0.0\n",
    "        for (imgs, lbls) in train_loader:\n",
    "            imgs = imgs.to(device = dev) # move images to dev (cpu or cuda)\n",
    "            lbls = lbls.to(device = dev) # move labels to dev (cpu or cuda)\n",
    "            outs = model(imgs) \n",
    "            loss = loss_fn(outs, lbls)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        if epoch%10 == 0:\n",
    "            print('{} Epoch {}, train loss {}'.format(\n",
    "                datetime.datetime.now(), epoch, train_loss/len(train_loader)))\n",
    "\n",
    "model = Net().to(device = dev)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-2)\n",
    "\n",
    "training_loop(n_epochs = 100,\n",
    "              model = model,\n",
    "              loss_fn = loss_fn,\n",
    "              optimizer = optimizer,\n",
    "              train_loader = train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0417a5d2-4098-4ee9-9200-f6ec99bb25f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[432, 16, 1152, 8, 16384, 32, 64, 2]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NetWidth(nn.Module):\n",
    "    def __init__(self, n_chann = 32):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, n_chann, kernel_size = 3, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(n_chann, n_chann // 2, kernel_size = 3, padding = 1)\n",
    "        self.fc1 = nn.Linear(8*8*n_chann//2,32)\n",
    "        self.fc2 = nn.Linear(32,2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh((self.conv1(x))), 2)\n",
    "        out = F.max_pool2d(torch.tanh((self.conv2(out))), 2)\n",
    "        out = out.view(-1, 8*8*n_chann // 2)\n",
    "        out = self.fc2(torch.tanh(self.fc1(out)))\n",
    "        return out\n",
    "model = NetWidth(16)\n",
    "[p.numel() for p in model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43dd50cd-74db-486a-914d-66f49f3becc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 14:09:12.409421 Epoch 0, train loss 0.5805089671141023\n",
      "2022-04-05 14:09:35.028305 Epoch 10, train loss 0.32748325966346037\n",
      "2022-04-05 14:09:58.404968 Epoch 20, train loss 0.2917436910852505\n",
      "2022-04-05 14:10:21.394728 Epoch 30, train loss 0.26867887263844725\n",
      "2022-04-05 14:10:44.360163 Epoch 40, train loss 0.25078140161219675\n",
      "2022-04-05 14:11:07.509703 Epoch 50, train loss 0.23527293827882997\n",
      "2022-04-05 14:11:30.188294 Epoch 60, train loss 0.22101993882542204\n",
      "2022-04-05 14:11:52.883772 Epoch 70, train loss 0.20760665644126333\n",
      "2022-04-05 14:12:15.666325 Epoch 80, train loss 0.19445801383940278\n",
      "2022-04-05 14:12:39.558623 Epoch 90, train loss 0.18103404518715135\n",
      "2022-04-05 14:13:02.886291 Epoch 100, train loss 0.16763439323681933\n"
     ]
    }
   ],
   "source": [
    "#L2 regularization: scaled sum of square of all weights and added to loss\n",
    "def training_loop_l2reg(n_epochs, loss_fn, model, optimizer, train_loader):\n",
    "    for epoch in range(n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, lbls in train_loader:\n",
    "            imgs = imgs.to(device = dev)\n",
    "            lbls = lbls.to(device = dev)\n",
    "            \n",
    "            outs = model(imgs)\n",
    "            loss = loss_fn(outs, lbls)\n",
    "            \n",
    "            l2_lambda = 0.0001  #scale\n",
    "            l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "            loss += l2_lambda * l2_norm\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "        if epoch%10 == 0:\n",
    "            print('{} Epoch {}, train loss {}'.format(\n",
    "                datetime.datetime.now(), epoch, loss_train/len(train_loader)))\n",
    "\n",
    "model = Net().to(device = dev)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-2)\n",
    "\n",
    "training_loop_l2reg(n_epochs = 100,\n",
    "              model = model,\n",
    "              loss_fn = loss_fn,\n",
    "              optimizer = optimizer,\n",
    "              train_loader = train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3af155c-7097-4855-acf4-0c08b8e385df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 14:34:49.384369 Epoch 0, train loss 0.6061132817890993\n",
      "2022-04-05 14:35:12.893912 Epoch 10, train loss 0.43156824370098723\n",
      "2022-04-05 14:35:38.898515 Epoch 20, train loss 0.3860600823239916\n",
      "2022-04-05 14:36:03.027098 Epoch 30, train loss 0.3711202098115994\n",
      "2022-04-05 14:36:25.973815 Epoch 40, train loss 0.3539815061031633\n",
      "2022-04-05 14:36:48.845791 Epoch 50, train loss 0.34695935875746853\n",
      "2022-04-05 14:37:11.915151 Epoch 60, train loss 0.33926434008179196\n",
      "2022-04-05 14:37:34.768879 Epoch 70, train loss 0.3253439414273402\n",
      "2022-04-05 14:37:57.604947 Epoch 80, train loss 0.32017058475761656\n",
      "2022-04-05 14:38:20.486793 Epoch 90, train loss 0.3082927555605105\n",
      "2022-04-05 14:38:43.533714 Epoch 100, train loss 0.3095439202656412\n"
     ]
    }
   ],
   "source": [
    "#Dropout: randomly zero out part of outputs in each iteration across nn\n",
    "class NetDropout(nn.Module):\n",
    "    def __init__(self, n_chann = 32):\n",
    "        super().__init__()\n",
    "        self.n_chann = n_chann\n",
    "        self.conv1 = nn.Conv2d(3, n_chann, kernel_size = 3, padding = 1)\n",
    "        self.conv1_dropout = nn.Dropout2d(p = 0.4) # zero each output with probability 0.4\n",
    "        self.conv2 = nn.Conv2d(n_chann, n_chann // 2, kernel_size = 3, padding = 1)\n",
    "        self.conv2_dropout = nn.Dropout2d(p = 0.4)\n",
    "        self.fc1 = nn.Linear(8*8*n_chann//2, 32)\n",
    "        self.fc2 = nn.Linear(32,2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = self.conv1_dropout(out)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = self.conv2_dropout(out)\n",
    "        out = out.view(-1, 8*8*self.n_chann//2)\n",
    "        out = self.fc2(torch.tanh(self.fc1(out)))\n",
    "        return out\n",
    "\n",
    "model = NetDropout(16).to(device = dev)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-2)\n",
    "\n",
    "training_loop(n_epochs = 100,\n",
    "              model = model,\n",
    "              loss_fn = loss_fn,\n",
    "              optimizer = optimizer,\n",
    "              train_loader = train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ed0e99f-9c2b-43ed-82f8-f9e66734de26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 16:38:39.073948 Epoch 0, train loss 0.6956953121598359\n",
      "2022-04-05 16:38:59.425686 Epoch 10, train loss 0.38025894998365145\n",
      "2022-04-05 16:39:19.376260 Epoch 20, train loss 0.3254766572432913\n",
      "2022-04-05 16:39:39.419141 Epoch 30, train loss 0.30201543108293205\n",
      "2022-04-05 16:39:59.355563 Epoch 40, train loss 0.2807615981170326\n",
      "2022-04-05 16:40:19.626337 Epoch 50, train loss 0.26131037940644913\n",
      "2022-04-05 16:40:39.941518 Epoch 60, train loss 0.24451454130897096\n",
      "2022-04-05 16:40:59.911288 Epoch 70, train loss 0.22867406567760334\n",
      "2022-04-05 16:41:20.573256 Epoch 80, train loss 0.21433308896175615\n",
      "2022-04-05 16:41:40.340989 Epoch 90, train loss 0.20088282118367543\n",
      "2022-04-05 16:42:00.257553 Epoch 100, train loss 0.18816859797117816\n"
     ]
    }
   ],
   "source": [
    "#add more depth\n",
    "class NetDepth(nn.Module):\n",
    "    def __init__(self, n_chann = 32):\n",
    "        super().__init__()\n",
    "        self.n_chann = n_chann\n",
    "        self.conv1 = nn.Conv2d(3, n_chann, kernel_size = 3, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(n_chann, n_chann//2, kernel_size = 3, padding = 1)\n",
    "        self.conv3 = nn.Conv2d(n_chann//2, n_chann//2, kernel_size = 3, padding = 1)\n",
    "        self.fc1 = nn.Linear(4*4*n_chann//2, 32)\n",
    "        self.fc2 = nn.Linear(32,2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)), 2)\n",
    "        out = out.view(-1, 4*4*self.n_chann//2)\n",
    "        out = self.fc2(torch.relu(self.fc1(out)))\n",
    "        return out\n",
    "\n",
    "model = NetDepth(16).to(device = dev)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-2)\n",
    "\n",
    "training_loop(n_epochs = 100,\n",
    "              model = model,\n",
    "              loss_fn = loss_fn,\n",
    "              optimizer = optimizer,\n",
    "              train_loader = train_loader)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd3fbb44-7467-4b10-a4ef-ee1fcb062257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 16:42:08.886536 Epoch 0, train loss 0.6438219843396715\n",
      "2022-04-05 16:42:29.026617 Epoch 10, train loss 0.3386854946043841\n",
      "2022-04-05 16:42:49.415523 Epoch 20, train loss 0.3004460634699293\n",
      "2022-04-05 16:43:08.965825 Epoch 30, train loss 0.2698589261550053\n",
      "2022-04-05 16:43:28.393318 Epoch 40, train loss 0.24464732845118092\n",
      "2022-04-05 16:43:47.953906 Epoch 50, train loss 0.22423920424500848\n",
      "2022-04-05 16:44:07.331254 Epoch 60, train loss 0.20699427618532423\n",
      "2022-04-05 16:44:26.699016 Epoch 70, train loss 0.19021810192591065\n",
      "2022-04-05 16:44:46.380023 Epoch 80, train loss 0.17443141274771112\n",
      "2022-04-05 16:45:05.883567 Epoch 90, train loss 0.1595998618300933\n",
      "2022-04-05 16:45:25.563944 Epoch 100, train loss 0.14543116014379603\n"
     ]
    }
   ],
   "source": [
    "#add more depth: Add skip connection:\n",
    "#the addition of an earlier output to a downstream output to emphasize on its effect, when computing the loss gradient\n",
    "class NetRes(nn.Module):\n",
    "    def __init__(self, n_chann = 32):\n",
    "        super().__init__()\n",
    "        self.n_chann = n_chann\n",
    "        self.conv1 = nn.Conv2d(3, n_chann, kernel_size = 3, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(n_chann, n_chann//2, kernel_size = 3, padding = 1)\n",
    "        self.conv3 = nn.Conv2d(n_chann//2, n_chann//2, kernel_size = 3, padding = 1)\n",
    "        self.fc1 = nn.Linear(4*4*n_chann//2, 32)\n",
    "        self.fc2 = nn.Linear(32,2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out1 = out\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)) + out1, 2)\n",
    "        out = out.view(-1, 4*4*self.n_chann//2)\n",
    "        out = self.fc2(torch.relu(self.fc1(out)))\n",
    "        return out\n",
    "model = NetRes(16).to(device = dev)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-2)\n",
    "\n",
    "training_loop(n_epochs = 100,\n",
    "              model = model,\n",
    "              loss_fn = loss_fn,\n",
    "              optimizer = optimizer,\n",
    "              train_loader = train_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d547c15a-61a0-4a01-b187-99c67c7c8d55",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn.init' has no attribute 'kaiming_nomral_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9c/5pww0nhx0bv_j4sxtbhpp24c0000gn/T/ipykernel_1921/2931525499.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNetDeep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/9c/5pww0nhx0bv_j4sxtbhpp24c0000gn/T/ipykernel_1921/2931525499.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_chann, n_blocks)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_chann\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_chann\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_chann\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_blocks\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mResBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_chann\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_chann\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn_chann\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/9c/5pww0nhx0bv_j4sxtbhpp24c0000gn/T/ipykernel_1921/2931525499.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_chann)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_chann\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_chann\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_chann\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_nomral_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonlinearity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn.init' has no attribute 'kaiming_nomral_'"
     ]
    }
   ],
   "source": [
    "#building very deep nn\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_chann):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(n_chann, n_chann, kernel_size = 3, padding = 1, bias = False)\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features = n_chann)\n",
    "        torch.nn.init.kaiming_nomral_(self.conv.weight, nonlinearity='relu')\n",
    "        torch.nn.constant_(self.batch_norm.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = torch.relu(out)\n",
    "        return out + x #skip\n",
    "\n",
    "class ResNetDeep(nn.Module):\n",
    "    def __init__(self, n_chann = 32, n_blocks = 10):\n",
    "        super().__init__()\n",
    "        self.n_chann = n_chann\n",
    "        self.conv1 = nn.Conv2d(3, n_chann, kernel_size = 3, padding = 1)\n",
    "        self.resblocks = nn.Sequential(*(n_blocks * [ResBlock(n_chann = n_chann)]))\n",
    "        self.fc1 = nn.Linear(8*8*n_chann, 32)\n",
    "        self.fc2 = nn.Linear(32,2)\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.resblocks(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 8*8*self.n_chann)\n",
    "        out = self.fc2(torch.relu(self.fc1(out)))\n",
    "        return out\n",
    "    \n",
    "model = ResNetDeep(16).to(device = dev)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-2)\n",
    "\n",
    "training_loop(n_epochs = 100,\n",
    "              model = model,\n",
    "              loss_fn = loss_fn,\n",
    "              optimizer = optimizer,\n",
    "              train_loader = train_loader) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
